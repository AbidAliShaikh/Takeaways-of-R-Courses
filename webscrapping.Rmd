---
title: "WebScrapping"
author: "Abid Ali Shaikh"
date: "2023-07-08"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
      version: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r webscrapping}
library(xml2)
library(httr)
library(rvest)
library(dplyr)

r <- read_html("https://datatrail-jhu.github.io/stable_website/webscrape.html")
s <- html_nodes(r,"strong")
t <- html_text(s)
print(t)

r2 <- read_html("https://www.bbc.com")
s2 <- html_nodes(r2,".media__link") # can be tracked using extension named:s
t2 <- html_text(s2)
print(trimws(t2))
##################################################

gitresp <- GET("https://api.github.com/users/abidalishaikh/repos")
gitcontent <- content(gitresp)

lapply(gitcontent,function(x){
  df <- data_frame(repo = x$name,
                   address = x$html_url)}) %>%
  bind_rows()
##########################################

surv<- GET("https://raw.githubusercontent.com/fivethirtyeight/data/master/steak-survey/steak-risk-survey.csv")
df_surv <- content(surv,type="text/csv")
df_surv
###### OR WE CAN ALSO USE READ_CSV

#downloading many files for offline scrapping
download.file("https://www.ibm.com/", destfile = "ibm.html")
root_nod1 <- rvest::read_html("ibm.html")
# #root_html <- rvest::read_html(root_nod1,"html")
# body_nod <- rvest::read_html(root_nod1,"body")
# p_nod <- rvest::read_html(body_nod,"p")
# p_content <- read_text(p_nod)
########################
table_nod <- rvest::html_nodes(root_nod1, "table")
c_dataframe <- html_table(table_nod)
c_dataframe
```
